{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGs1D81AB/M3A4gDPK44ZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xodud5654/redwoods_final/blob/main/Untitled30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnrGxQiKucEW",
        "outputId": "43d06075-5e00-4acf-e426-85b0700d2786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-14 03:53:13--  http://chaos.inje.ac.kr:3030/data/chest_xray_data.zip\n",
            "Resolving chaos.inje.ac.kr (chaos.inje.ac.kr)... 203.241.251.51\n",
            "Connecting to chaos.inje.ac.kr (chaos.inje.ac.kr)|203.241.251.51|:3030... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1225713864 (1.1G) [application/zip]\n",
            "Saving to: ‘chest_xray_data.zip’\n",
            "\n",
            "chest_xray_data.zip  45%[========>           ] 529.54M  1.36MB/s    eta 4m 30s "
          ]
        }
      ],
      "source": [
        "!wget http://chaos.inje.ac.kr:3030/data/chest_xray_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chest_xray_data.zip   # -d chest_xray_data\n"
      ],
      "metadata": {
        "id": "CDN2Zen6ugHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.image import imread\n",
        "import tensorflow\n",
        "from tensorflow.keras import models, layers\n"
      ],
      "metadata": {
        "id": "4mp4IC7IukGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting up the data\n",
        "\n",
        "## Set the image size \n",
        "IMG_SIZE = (224, 224)\n",
        "batchSize = 32\n",
        "\n",
        "print(\"Training Images:\")\n",
        "train_data = tensorflow.keras.preprocessing.image_dataset_from_directory(directory = train_dir,\n",
        "                                                                 image_size = IMG_SIZE,\n",
        "                                                                 label_mode = \"binary\",\n",
        "                                                                 color_mode = \"rgb\",\n",
        "                                                                 batch_size = batchSize)\n",
        "\n",
        "print(\"Testing Images:\")\n",
        "test_data = tensorflow.keras.preprocessing.image_dataset_from_directory(directory = test_dir,\n",
        "                                                                 image_size = IMG_SIZE,\n",
        "                                                                 label_mode = \"binary\",\n",
        "                                                                 color_mode = \"rgb\",\n",
        "                                                                 batch_size = batchSize)\n",
        "\n",
        "print(\"Validation Images:\")\n",
        "val_data = tensorflow.keras.preprocessing.image_dataset_from_directory(directory = validation_dir,\n",
        "                                                                 image_size = IMG_SIZE,\n",
        "                                                                 label_mode = \"binary\",\n",
        "                                                                 color_mode = \"rgb\",\n",
        "                                                                 batch_size = batchSize)"
      ],
      "metadata": {
        "id": "D11UqU9UumtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Data augmentation\n",
        "#\n",
        "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "#\n",
        "tensorflow.get_logger().setLevel('ERROR')  # Clear warnings in data augmentation\n",
        "\n",
        "from tensorflow import keras\n",
        "data_augmentation = keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomZoom(0.2),\n",
        "  layers.RandomHeight(0.2),  # Not compatible with model\n",
        "  layers.RandomWidth(0.2),\n",
        "  # layers.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n",
        "], name =\"data_augmentation\")\n"
      ],
      "metadata": {
        "id": "1s9xt08Surss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from matplotlib.image import imread\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import models, layers\n",
        "\n",
        "base_model = keras.applications.resnet50.ResNet50(include_top = False)\n",
        "base_model.trainable = True # False\n",
        "\n",
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 85  # half of the whole layers\n",
        "\n",
        "#  Fine-tuning after layer_number larger than 170\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False"
      ],
      "metadata": {
        "id": "69uztGDAuyo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Input(shape=(224,224,3),name='input_layer'),\n",
        "  # layers.Rescaling(1./255),\n",
        "  # data_augmentation,\n",
        "  base_model,\n",
        "  layers.GlobalMaxPooling2D(name = \"global_max\"),\n",
        "  layers.Dense(128,activation='relu'),\n",
        "  layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), \n",
        "              loss = 'binary_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Gil8CAEZu3MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "kuq0mzvAu5Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=10) # if val loss decreases for 10 epochs in a row, stop training\n",
        "# Creating learning rate reduction callback\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.25, # multiply the learning rate by 0.2 (reduce by 4x)\n",
        "                                                 patience=4,  # 3,4,5\n",
        "                                                 verbose=1, # print out when learning rate goes down \n",
        "                                                 min_lr=1e-7)\n",
        "# Check the best model and save the best model\n",
        "cp_callback = keras.callbacks.ModelCheckpoint(filepath=\"./model/chest_xray.hdf5\", \n",
        "                              monitor='val_accuracy', verbose=0, save_best_only=True)\n",
        "## Check the summary\n",
        "for no, layer in enumerate(model.layers):\n",
        "  print(no, layer.trainable)\n",
        "\n",
        "history = model.fit(train_data, \n",
        "                    epochs=1, \n",
        "                    steps_per_epoch = len(train_data), \n",
        "                    validation_data = test_data,\n",
        "                    validation_steps = len(test_data), # batchSize,\n",
        "                    callbacks = [cp_callback, early_stopping, reduce_lr])"
      ],
      "metadata": {
        "id": "T-q5upfxu7cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "model_best = load_model('./model/chest_xray.hdf5')\n",
        "# model_best.evaluate(X_test, y_test)  #, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Correct-handling with tf_dataset - test_data,unbatch()\n",
        "import tensorflow as tf\n",
        "\n",
        "y_test=[]\n",
        "y_pred=[]\n",
        "# X_test = tf.zeros(shape=(128,128,3), name=None)  #\n",
        "X_test = np.empty((224,224,3), int)"
      ],
      "metadata": {
        "id": "NL5RE9wcu9j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best model로 전체 테스트 batch에 대하여 다시 예측을 실시함.\n",
        "for image, label in test_data.unbatch():   # 개개의 이미지에 대한 예측과 label 수집하면서 test 데이터를 X_test 로 저장\n",
        "    # print(images.shape,labels.shape)\n",
        "    y_pred0 = model_best.predict(np.expand_dims(image, axis=0), verbose=0) #, batch_size=1)  \n",
        "    y_pred.append(np.round(y_pred0[0]))  #, axis=0)\n",
        "    y_test.append(label.numpy())\n",
        "    X_test = np.vstack([X_test,image])  #, axis=0)\n",
        "#     tf.stack([X_test, image], axis=0)\n",
        "    # break\n",
        "\n",
        "# X_test = X_test.reshape(-1,128,128,3)\n",
        "X_test=X_test[1:]\n",
        "y_test = [int(i) for i in y_test]\n",
        "y_pred = [int(i) for i in y_pred]\n",
        "print(len(y_test),len(y_pred), X_test.shape)\n",
        "# print(y_test.shape,y_pred.shape)\n",
        "\n",
        "print(\"정답=\", y_test[0])\n",
        "print(\"예측값=\", y_pred[0]) #, np.round(y_pred0[0]))"
      ],
      "metadata": {
        "id": "KMtX4VbzvBY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm, cbar=False, xticklabels=['NORMAL', 'PNEUMONIA'], yticklabels=['NORMAL', 'PNEUMONIA'], fmt='d', annot=True, cmap=plt.cm.Greens) #coolwarm)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion matrix, Best ACC={round(acc,3)}', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PodEuvgovDjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}